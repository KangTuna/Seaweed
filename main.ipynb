{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rectangle(img: np.array, js: dict) -> np.array:\n",
    "    # 사각형 좌표 구하기\n",
    "    top,bot = (js['top_x'],js['top_y']),(js['bot_x'],js['bot_y'])\n",
    "\n",
    "    # 사각형 그리기\n",
    "    cv2.rectangle(img,top,bot,(255,255,255)) # Grayscale 이라서 흰색으로 그려야됨\n",
    "    \n",
    "    return img\n",
    "\n",
    "def otsu_algorithm(img: np.array) -> np.array:\n",
    "    ret, otsu = cv2.threshold(img,-1,255,cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    return otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train valid 파일 경로\n",
    "train_json_path = './dataset/train/train_defected_json'\n",
    "train_img_path = './dataset/train/train_defected_dataset'\n",
    "\n",
    "valid_json_path = './dataset/validation/validation_defected_json'\n",
    "valid_img_path = './dataset/validation/validation_defected_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지에 Bbox 그리기\n",
    "## TrainData \n",
    "\n",
    "# make dir\n",
    "if not os.path.isdir('./dataset/Bboximages/train'):\n",
    "    os.makedirs('./dataset/Bboximages/train/st')\n",
    "    os.makedirs('./dataset/Bboximages/train/aq')\n",
    "    os.makedirs('./dataset/Bboximages/train/fl')\n",
    "\n",
    "for idx in os.listdir(train_json_path):\n",
    "    # json files load\n",
    "    with open(f'{train_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}')\n",
    "\n",
    "    # draw rectangle\n",
    "    make_rectangle(img, json_dict)\n",
    "\n",
    "    # save image\n",
    "    cv2.imwrite(f'./dataset/Bboximages/train/{json_dict[\"defect_class\"]}/{json_dict[\"image_name\"]}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이미지 그레이스케일로 가져와서 오츠 알고리즘 돌리기\n",
    "## TrainData \n",
    "\n",
    "# make dir\n",
    "if not os.path.isdir('./dataset/Bboximages/train'):\n",
    "    os.makedirs('./dataset/Bboximages/train/st')\n",
    "    os.makedirs('./dataset/Bboximages/train/aq')\n",
    "    os.makedirs('./dataset/Bboximages/train/fl')\n",
    "\n",
    "for idx in os.listdir(train_json_path):\n",
    "    # json files load\n",
    "    with open(f'{train_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # apply otsu algorithm\n",
    "    img = otsu_algorithm(img)\n",
    "\n",
    "    # draw rectangle\n",
    "    make_rectangle(img, json_dict)\n",
    "\n",
    "    # save image\n",
    "    cv2.imwrite(f'./dataset/Bboximages/train/{json_dict[\"defect_class\"]}/{json_dict[\"image_name\"]}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RGB 채널 쪼개서 오츠 알고리즘 적용하고 다시 합치기\n",
    "## TrainData \n",
    "\n",
    "# make dir\n",
    "if not os.path.isdir('./dataset/Otsu/train'):\n",
    "    os.makedirs('./dataset/Otsu/train/st')\n",
    "    os.makedirs('./dataset/Otsu/train/aq')\n",
    "    os.makedirs('./dataset/Otsu/train/fl')\n",
    "\n",
    "for idx in os.listdir(train_json_path):\n",
    "    # json files load\n",
    "    with open(f'{train_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image BGR\n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}')\n",
    "\n",
    "    # RGB채널 쪼개기\n",
    "    B,G,R = cv2.split(img)\n",
    "\n",
    "    # 오츠 알고리즘 적용후 이미지 or 연산으로 합치기\n",
    "    B,G,R = map(otsu_algorithm,[B,G,R])\n",
    "    temp = cv2.bitwise_or(B,G)\n",
    "    img = cv2.bitwise_or(temp,R)\n",
    "\n",
    "    # draw rectangle\n",
    "    make_rectangle(img, json_dict)\n",
    "\n",
    "    # save image\n",
    "    # cv2.imwrite(f'./dataset/Otsu/train/{json_dict[\"defect_class\"]}/{json_dict[\"image_name\"]}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation data \n",
    "\n",
    "# make dir\n",
    "if not os.path.isdir('./dataset/Bboximages/validation'):\n",
    "    os.makedirs('./dataset/Bboximages/validation/st')\n",
    "    os.makedirs('./dataset/Bboximages/validation/aq')\n",
    "    os.makedirs('./dataset/Bboximages/validation/fl')\n",
    "\n",
    "for idx in os.listdir(valid_json_path):\n",
    "    # json files load\n",
    "    with open(f'{valid_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(f'{valid_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # apply otsu algorithm\n",
    "    img = otsu_algorithm(img)\n",
    "\n",
    "    # draw rectangle\n",
    "    make_rectangle(img, json_dict)\n",
    "\n",
    "    # save image\n",
    "    cv2.imwrite(f'./dataset/Bboximages/validation/{json_dict[\"defect_class\"]}/{json_dict[\"image_name\"]}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation data \n",
    "\n",
    "# make dir\n",
    "if not os.path.isdir('./dataset/Otsu/validation'):\n",
    "    os.makedirs('./dataset/Otsu/validation/st')\n",
    "    os.makedirs('./dataset/Otsu/validation/aq')\n",
    "    os.makedirs('./dataset/Otsu/validation/fl')\n",
    "\n",
    "for idx in os.listdir(valid_json_path):\n",
    "    # json files load\n",
    "    with open(f'{valid_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image BGR\n",
    "    img = cv2.imread(f'{valid_img_path}/{json_dict[\"image_name\"]}')\n",
    "\n",
    "    # RGB채널 쪼개기\n",
    "    B,G,R = cv2.split(img)\n",
    "\n",
    "    # 오츠 알고리즘 적용후 이미지 or 연산으로 합치기\n",
    "    B,G,R = map(otsu_algorithm,[B,G,R])\n",
    "    temp = cv2.bitwise_or(B,G)\n",
    "    img = cv2.bitwise_or(temp,R)\n",
    "\n",
    "    # draw rectangle\n",
    "    make_rectangle(img, json_dict)\n",
    "\n",
    "    # save image\n",
    "    cv2.imwrite(f'./dataset/Bboximages/validation/{json_dict[\"defect_class\"]}/{json_dict[\"image_name\"]}',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bbox의 모든 픽셀 값을 더해서 검출 되는지 안되는지 확인\n",
    "# 이진 알고리즘 돌렸을때 검출이 된다면 Bbox 내에 반드시 흰색이 있을것이다 라는 가정\n",
    "def ROI_sum(img: np.array,js: dict) -> int:\n",
    "    img = img[js['top_y']:js['bot_y'], js['top_x']:js['bot_x']]\n",
    "    return sum(sum(img))\n",
    "\n",
    "valid_defected_fail = []\n",
    "train_defected_fail = []\n",
    "\n",
    "# train 데이터에서 Bbox 체크\n",
    "for idx in os.listdir(train_json_path):\n",
    "    # json files load\n",
    "    with open(f'{train_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image BGR\n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}')\n",
    "\n",
    "    # RGB채널 쪼개기\n",
    "    B,G,R = cv2.split(img)\n",
    "\n",
    "    # 오츠 알고리즘 적용후 이미지 or 연산으로 합치기\n",
    "    B,G,R = map(otsu_algorithm,[B,G,R])\n",
    "    temp = cv2.bitwise_or(B,G)\n",
    "    img = cv2.bitwise_or(temp,R)\n",
    "\n",
    "    # sum of all pixels\n",
    "    pixel = ROI_sum(img, json_dict)\n",
    "\n",
    "    # check defect\n",
    "    if not pixel:\n",
    "        train_defected_fail.append(idx)\n",
    "\n",
    "# validation 데이터에서 Bbox 체크 \n",
    "for idx in os.listdir(valid_json_path):\n",
    "    # json files load\n",
    "    with open(f'{valid_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image BGR\n",
    "    img = cv2.imread(f'{valid_img_path}/{json_dict[\"image_name\"]}')\n",
    "\n",
    "    # RGB채널 쪼개기\n",
    "    B,G,R = cv2.split(img)\n",
    "\n",
    "    # 오츠 알고리즘 적용후 이미지 or 연산으로 합치기\n",
    "    B,G,R = map(otsu_algorithm,[B,G,R])\n",
    "    temp = cv2.bitwise_or(B,G)\n",
    "    img = cv2.bitwise_or(temp,R)\n",
    "\n",
    "    # sum of all pixels\n",
    "    pixel = ROI_sum(img, json_dict)\n",
    "\n",
    "    # check defect\n",
    "    if not pixel:\n",
    "        valid_defected_fail.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004\n"
     ]
    }
   ],
   "source": [
    "fail_count = 0\n",
    "for fail in train_defected_fail:\n",
    "    with open(f'{train_json_path}/{fail}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "    ret,tri = cv2.threshold(img,-1,255,cv2.THRESH_BINARY | cv2.THRESH_TRIANGLE)\n",
    "    \n",
    "    pixel = ROI_sum(tri, json_dict)\n",
    "\n",
    "    if not pixel:\n",
    "        fail_count += 1\n",
    "\n",
    "for fail in valid_defected_fail:\n",
    "    with open(f'{valid_json_path}/{fail}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    img = cv2.imread(f'{valid_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "    ret,tri = cv2.threshold(img,-1,255,cv2.THRESH_BINARY | cv2.THRESH_TRIANGLE)\n",
    "    \n",
    "    pixel = ROI_sum(tri, json_dict)\n",
    "\n",
    "    if not pixel:\n",
    "        fail_count += 1\n",
    "\n",
    "print(fail_count/3750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seaweed_01257.json',\n",
       " 'seaweed_01261.json',\n",
       " 'seaweed_01282.json',\n",
       " 'seaweed_01283.json',\n",
       " 'seaweed_01284.json',\n",
       " 'seaweed_01298.json',\n",
       " 'seaweed_01302.json',\n",
       " 'seaweed_01323.json',\n",
       " 'seaweed_01330.json',\n",
       " 'seaweed_01333.json',\n",
       " 'seaweed_01353.json',\n",
       " 'seaweed_01356.json',\n",
       " 'seaweed_01379.json',\n",
       " 'seaweed_01387.json',\n",
       " 'seaweed_01394.json',\n",
       " 'seaweed_01396.json',\n",
       " 'seaweed_01432.json',\n",
       " 'seaweed_01454.json',\n",
       " 'seaweed_01498.json',\n",
       " 'seaweed_01513.json',\n",
       " 'seaweed_01523.json',\n",
       " 'seaweed_01528.json',\n",
       " 'seaweed_01557.json',\n",
       " 'seaweed_01579.json',\n",
       " 'seaweed_01587.json',\n",
       " 'seaweed_01622.json',\n",
       " 'seaweed_01631.json',\n",
       " 'seaweed_01637.json',\n",
       " 'seaweed_01651.json',\n",
       " 'seaweed_01679.json',\n",
       " 'seaweed_01682.json',\n",
       " 'seaweed_01683.json',\n",
       " 'seaweed_01691.json',\n",
       " 'seaweed_01696.json',\n",
       " 'seaweed_01697.json',\n",
       " 'seaweed_01717.json',\n",
       " 'seaweed_01721.json',\n",
       " 'seaweed_01725.json',\n",
       " 'seaweed_01742.json',\n",
       " 'seaweed_01743.json',\n",
       " 'seaweed_01772.json',\n",
       " 'seaweed_01815.json',\n",
       " 'seaweed_01838.json',\n",
       " 'seaweed_01842.json',\n",
       " 'seaweed_01853.json',\n",
       " 'seaweed_01864.json',\n",
       " 'seaweed_01870.json',\n",
       " 'seaweed_01896.json',\n",
       " 'seaweed_01903.json',\n",
       " 'seaweed_01948.json',\n",
       " 'seaweed_02002.json',\n",
       " 'seaweed_02017.json',\n",
       " 'seaweed_02018.json',\n",
       " 'seaweed_02020.json',\n",
       " 'seaweed_02031.json',\n",
       " 'seaweed_02033.json',\n",
       " 'seaweed_02036.json',\n",
       " 'seaweed_02048.json',\n",
       " 'seaweed_02051.json',\n",
       " 'seaweed_02060.json',\n",
       " 'seaweed_02088.json',\n",
       " 'seaweed_02105.json',\n",
       " 'seaweed_02112.json',\n",
       " 'seaweed_02150.json',\n",
       " 'seaweed_02173.json',\n",
       " 'seaweed_02187.json',\n",
       " 'seaweed_02201.json',\n",
       " 'seaweed_02217.json',\n",
       " 'seaweed_02230.json',\n",
       " 'seaweed_02231.json',\n",
       " 'seaweed_02236.json',\n",
       " 'seaweed_02276.json',\n",
       " 'seaweed_02326.json',\n",
       " 'seaweed_02374.json',\n",
       " 'seaweed_02382.json',\n",
       " 'seaweed_02387.json',\n",
       " 'seaweed_02389.json',\n",
       " 'seaweed_02416.json',\n",
       " 'seaweed_02438.json',\n",
       " 'seaweed_02460.json',\n",
       " 'seaweed_02467.json',\n",
       " 'seaweed_02469.json',\n",
       " 'seaweed_02475.json',\n",
       " 'seaweed_02538.json',\n",
       " 'seaweed_02540.json',\n",
       " 'seaweed_02543.json',\n",
       " 'seaweed_02548.json',\n",
       " 'seaweed_02551.json',\n",
       " 'seaweed_02553.json',\n",
       " 'seaweed_02555.json',\n",
       " 'seaweed_02558.json',\n",
       " 'seaweed_02574.json',\n",
       " 'seaweed_02582.json',\n",
       " 'seaweed_02592.json',\n",
       " 'seaweed_02625.json',\n",
       " 'seaweed_02627.json',\n",
       " 'seaweed_02636.json',\n",
       " 'seaweed_02637.json',\n",
       " 'seaweed_02638.json',\n",
       " 'seaweed_02642.json',\n",
       " 'seaweed_02699.json',\n",
       " 'seaweed_02703.json',\n",
       " 'seaweed_02707.json',\n",
       " 'seaweed_02723.json',\n",
       " 'seaweed_02745.json',\n",
       " 'seaweed_02772.json',\n",
       " 'seaweed_02775.json',\n",
       " 'seaweed_02780.json',\n",
       " 'seaweed_02824.json',\n",
       " 'seaweed_02828.json',\n",
       " 'seaweed_02833.json',\n",
       " 'seaweed_02853.json',\n",
       " 'seaweed_02868.json',\n",
       " 'seaweed_02879.json',\n",
       " 'seaweed_02886.json',\n",
       " 'seaweed_02890.json',\n",
       " 'seaweed_02933.json',\n",
       " 'seaweed_02936.json',\n",
       " 'seaweed_02942.json',\n",
       " 'seaweed_02949.json',\n",
       " 'seaweed_02955.json',\n",
       " 'seaweed_02964.json',\n",
       " 'seaweed_02965.json',\n",
       " 'seaweed_02976.json',\n",
       " 'seaweed_02983.json',\n",
       " 'seaweed_02988.json',\n",
       " 'seaweed_02990.json',\n",
       " 'seaweed_02996.json',\n",
       " 'seaweed_03004.json',\n",
       " 'seaweed_03018.json',\n",
       " 'seaweed_03031.json',\n",
       " 'seaweed_03041.json',\n",
       " 'seaweed_03043.json',\n",
       " 'seaweed_03049.json',\n",
       " 'seaweed_03063.json',\n",
       " 'seaweed_03083.json',\n",
       " 'seaweed_03098.json',\n",
       " 'seaweed_03102.json',\n",
       " 'seaweed_03118.json',\n",
       " 'seaweed_03122.json',\n",
       " 'seaweed_03144.json',\n",
       " 'seaweed_03150.json',\n",
       " 'seaweed_03157.json',\n",
       " 'seaweed_03183.json',\n",
       " 'seaweed_03184.json',\n",
       " 'seaweed_03190.json',\n",
       " 'seaweed_03194.json',\n",
       " 'seaweed_03212.json',\n",
       " 'seaweed_03225.json',\n",
       " 'seaweed_03233.json',\n",
       " 'seaweed_03253.json',\n",
       " 'seaweed_03258.json',\n",
       " 'seaweed_03263.json',\n",
       " 'seaweed_03283.json',\n",
       " 'seaweed_03295.json',\n",
       " 'seaweed_03317.json',\n",
       " 'seaweed_03346.json',\n",
       " 'seaweed_03357.json',\n",
       " 'seaweed_03396.json',\n",
       " 'seaweed_03418.json',\n",
       " 'seaweed_03457.json',\n",
       " 'seaweed_03474.json',\n",
       " 'seaweed_03491.json',\n",
       " 'seaweed_03497.json',\n",
       " 'seaweed_03512.json',\n",
       " 'seaweed_03522.json',\n",
       " 'seaweed_03536.json',\n",
       " 'seaweed_03548.json',\n",
       " 'seaweed_03561.json',\n",
       " 'seaweed_03580.json',\n",
       " 'seaweed_03590.json',\n",
       " 'seaweed_03608.json',\n",
       " 'seaweed_03631.json',\n",
       " 'seaweed_03633.json',\n",
       " 'seaweed_03634.json',\n",
       " 'seaweed_03646.json',\n",
       " 'seaweed_03647.json',\n",
       " 'seaweed_03672.json',\n",
       " 'seaweed_03673.json',\n",
       " 'seaweed_03681.json',\n",
       " 'seaweed_03702.json',\n",
       " 'seaweed_03712.json',\n",
       " 'seaweed_03725.json',\n",
       " 'seaweed_03762.json',\n",
       " 'seaweed_03796.json',\n",
       " 'seaweed_03815.json',\n",
       " 'seaweed_03844.json',\n",
       " 'seaweed_03852.json',\n",
       " 'seaweed_03859.json',\n",
       " 'seaweed_03862.json',\n",
       " 'seaweed_03886.json',\n",
       " 'seaweed_03897.json',\n",
       " 'seaweed_03909.json',\n",
       " 'seaweed_03933.json',\n",
       " 'seaweed_03959.json',\n",
       " 'seaweed_03963.json',\n",
       " 'seaweed_03976.json',\n",
       " 'seaweed_03977.json',\n",
       " 'seaweed_03988.json',\n",
       " 'seaweed_04006.json',\n",
       " 'seaweed_04016.json',\n",
       " 'seaweed_04064.json',\n",
       " 'seaweed_04071.json',\n",
       " 'seaweed_04072.json',\n",
       " 'seaweed_04076.json',\n",
       " 'seaweed_04083.json',\n",
       " 'seaweed_04086.json',\n",
       " 'seaweed_04094.json',\n",
       " 'seaweed_04097.json',\n",
       " 'seaweed_04101.json',\n",
       " 'seaweed_04104.json',\n",
       " 'seaweed_04110.json',\n",
       " 'seaweed_04120.json',\n",
       " 'seaweed_04123.json',\n",
       " 'seaweed_04163.json',\n",
       " 'seaweed_04205.json',\n",
       " 'seaweed_04206.json',\n",
       " 'seaweed_04207.json',\n",
       " 'seaweed_04215.json',\n",
       " 'seaweed_04238.json',\n",
       " 'seaweed_04251.json',\n",
       " 'seaweed_04255.json',\n",
       " 'seaweed_04258.json',\n",
       " 'seaweed_04263.json',\n",
       " 'seaweed_04278.json',\n",
       " 'seaweed_04284.json',\n",
       " 'seaweed_04285.json',\n",
       " 'seaweed_04299.json',\n",
       " 'seaweed_04312.json',\n",
       " 'seaweed_04326.json',\n",
       " 'seaweed_04399.json',\n",
       " 'seaweed_04410.json',\n",
       " 'seaweed_04447.json',\n",
       " 'seaweed_04455.json',\n",
       " 'seaweed_04459.json',\n",
       " 'seaweed_04473.json',\n",
       " 'seaweed_04478.json',\n",
       " 'seaweed_04504.json',\n",
       " 'seaweed_04520.json',\n",
       " 'seaweed_04524.json',\n",
       " 'seaweed_04528.json',\n",
       " 'seaweed_04537.json',\n",
       " 'seaweed_04558.json',\n",
       " 'seaweed_04560.json',\n",
       " 'seaweed_04571.json',\n",
       " 'seaweed_04575.json',\n",
       " 'seaweed_04605.json',\n",
       " 'seaweed_04651.json',\n",
       " 'seaweed_04652.json',\n",
       " 'seaweed_04669.json',\n",
       " 'seaweed_04672.json',\n",
       " 'seaweed_04685.json',\n",
       " 'seaweed_04686.json',\n",
       " 'seaweed_04693.json',\n",
       " 'seaweed_04708.json',\n",
       " 'seaweed_04711.json',\n",
       " 'seaweed_04714.json',\n",
       " 'seaweed_04722.json',\n",
       " 'seaweed_04738.json',\n",
       " 'seaweed_04739.json',\n",
       " 'seaweed_04827.json',\n",
       " 'seaweed_04836.json',\n",
       " 'seaweed_04849.json',\n",
       " 'seaweed_04853.json',\n",
       " 'seaweed_04854.json',\n",
       " 'seaweed_04858.json',\n",
       " 'seaweed_04876.json',\n",
       " 'seaweed_04884.json',\n",
       " 'seaweed_04896.json',\n",
       " 'seaweed_04898.json',\n",
       " 'seaweed_04904.json',\n",
       " 'seaweed_04930.json',\n",
       " 'seaweed_04956.json',\n",
       " 'seaweed_04984.json',\n",
       " 'seaweed_04987.json',\n",
       " 'seaweed_04997.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_defected_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "defected_fail = []\n",
    "\n",
    "for idx in os.listdir(train_json_path):\n",
    "    # json files load\n",
    "    with open(f'{train_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(f'{train_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    retval, otsu_thresh = cv2.threshold(img, -1, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    # 오츠 알고리즘으로 얻은 임계값을 기준으로 상한과 하한 설정\n",
    "    lower_thresh = 0.5 * retval  # 하한 임계값: 오츠 임계값의 50%\n",
    "    upper_thresh = retval         # 상한 임계값: 오츠 임계값 자체\n",
    "\n",
    "    # 가우시안 블러처리\n",
    "    blurred_image = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # 캐니엣지 검출\n",
    "    edges = cv2.Canny(blurred_image, threshold1=lower_thresh, threshold2=upper_thresh)\n",
    "\n",
    "    # sum of all pixels\n",
    "    pixel = ROI_sum(edges, json_dict)\n",
    "\n",
    "    # check defect\n",
    "    if not pixel:\n",
    "        defected_fail.append(idx)\n",
    "\n",
    "for idx in os.listdir(valid_json_path):\n",
    "    # json files load\n",
    "    with open(f'{valid_json_path}/{idx}', 'r') as f:\n",
    "        json_dict = json.load(f)\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(f'{valid_img_path}/{json_dict[\"image_name\"]}', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    retval, otsu_thresh = cv2.threshold(img, -1, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    # 오츠 알고리즘으로 얻은 임계값을 기준으로 상한과 하한 설정\n",
    "    lower_thresh = 0.5 * retval  # 하한 임계값: 오츠 임계값의 50%\n",
    "    upper_thresh = retval         # 상한 임계값: 오츠 임계값 자체\n",
    "\n",
    "    # 가우시안 블러처리\n",
    "    blurred_image = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "\n",
    "    # 캐니엣지 검출\n",
    "    edges = cv2.Canny(blurred_image, threshold1=lower_thresh, threshold2=upper_thresh)\n",
    "\n",
    "    # sum of all pixels\n",
    "    pixel = ROI_sum(edges, json_dict)\n",
    "\n",
    "    # check defect\n",
    "    if not pixel:\n",
    "        defected_fail.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0264"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(defected_fail)/3750"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIDATA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
